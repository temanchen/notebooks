{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression score: 0.938889\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, neighbors, linear_model\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "n_samples = len(X_digits)\n",
    "n_train = int(.9 * n_samples)\n",
    "\n",
    "X_train = X_digits[:n_train]\n",
    "y_train = y_digits[:n_train]\n",
    "X_test = X_digits[n_train:]\n",
    "y_test = y_digits[n_train:]\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "print('LogisticRegression score: %f' % logistic.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(loss_func, init_theta, alpha=0.5, max_iter=5000, min_diff=1e-4):\n",
    "    cur_theta = np.float64(init_theta)\n",
    "    (loss, grad) = loss_func(cur_theta)\n",
    "    n_iter = 0\n",
    "    while n_iter < max_iter and abs(loss) > min_diff:\n",
    "        cur_theta -= alpha * grad\n",
    "        (loss, grad) = loss_func(cur_theta)\n",
    "        n_iter += 1\n",
    "    return cur_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stochastic gradient descent\n",
    "def sgd(loss_func, init_theta, X, y, alpha=0.5, max_iter=500, n_samples=1000, min_diff=1e-4):\n",
    "    cur_theta = np.float64(init_theta)\n",
    "    loss_sum = np.float64(0)\n",
    "    count = 0\n",
    "    for _ in range(0, max_iter):\n",
    "        for i in range(0, len(X)):\n",
    "            (loss, grad) = loss_func(cur_theta, X[i], y[i])\n",
    "            loss_sum += loss\n",
    "            cur_theta -= alpha * grad\n",
    "            count += 1\n",
    "            if count == n_samples:\n",
    "                if abs(loss_sum / n_samples) < min_diff:\n",
    "                    return cur_theta\n",
    "                count = 0\n",
    "                loss_sum = np.float64(0)\n",
    "    return cur_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxRegression():\n",
    "    def __init__(self, use_sgd=True):\n",
    "        self.Theta = None\n",
    "        self.label2index = None\n",
    "        self.use_sgd = use_sgd\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        (m, n) = X_train.shape\n",
    "        X = np.c_[ np.ones(m), X_train ]\n",
    "        y = self.to_vector(y_train)\n",
    "        K = len(self.label2index)\n",
    "        init_theta = np.zeros([K, n+1])\n",
    "        if self.use_sgd:\n",
    "            self.Theta = sgd(self.loss_function_i, init_theta, X, y)\n",
    "        else:\n",
    "            loss_func = lambda theta: self.loss_function(theta, X, y)\n",
    "            self.Theta = gradient_descent(loss_func, init_theta)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        (m, n) = X_test.shape\n",
    "        X = np.c_[ np.ones(m), X_test ]\n",
    "        h = np.empty([m, len(self.label2index)])\n",
    "        for i in range(0, m):\n",
    "            powers = self.Theta @ X[i]\n",
    "            # subtract from the max power to avoid overflow\n",
    "            exps = np.exp(powers - np.max(powers))\n",
    "            probs = exps / np.sum(exps)\n",
    "            h[i][:] = probs\n",
    "        return h\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        (m, n) = X_test.shape\n",
    "        X = np.c_[ np.ones(m), X_test ]\n",
    "        n_correct = 0\n",
    "        index2label = { v: k for k, v in self.label2index.items() }\n",
    "        for i in range(0, m):\n",
    "            powers = self.Theta @ X[i]\n",
    "            exps = np.exp(powers - np.max(powers))\n",
    "            if y_test[i] == index2label[np.argmax(exps)]:\n",
    "                n_correct += 1\n",
    "        return n_correct / m\n",
    "    \n",
    "    # convert label to a column of identity matrix I_K\n",
    "    def to_vector(self, y, new_label=True):\n",
    "        if new_label:\n",
    "            y_list = np.unique(y).tolist()\n",
    "            self.label2index = { y_i: y_list.index(y_i) for y_i in y_list }\n",
    "        n_labels = len(y_list)\n",
    "        return np.array([ np.eye(n_labels)[self.label2index[y_i]] for y_i in y ])\n",
    "\n",
    "    def loss_function_i(self, Theta, x_i, y_i):\n",
    "        K = Theta.shape[0]\n",
    "        n = len(x_i)\n",
    "        powers = Theta @ x_i\n",
    "        # subtract from the max power to avoid overflow\n",
    "        exps = np.exp(powers - np.max(powers))\n",
    "        # take maximum of probs and a tiny value to avoid taking log of 0\n",
    "        probs = np.maximum(exps / np.sum(exps), np.finfo(float).tiny)\n",
    "        cost_i = -1 * y_i @ np.log(probs)\n",
    "        grad_i = -1 * (y_i - probs).reshape([K, 1]) @ x_i.reshape([1, n])\n",
    "        return (cost_i, grad_i)\n",
    "    \n",
    "    def loss_function(self, Theta, X, y):\n",
    "        m = X.shape[0]\n",
    "        cost = np.float64(0)\n",
    "        grad = np.zeros(Theta.shape)\n",
    "        for i in range(0, m):\n",
    "            (cost_i, grad_i) = self.loss_function_i(Theta, X[i], y[i])\n",
    "            cost += cost_i\n",
    "            grad += grad_i\n",
    "        return (cost / m, grad / m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SoftmaxRegression at 0x7f64ef7072e8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = SoftmaxRegression()\n",
    "softmax.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9277777777777778"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

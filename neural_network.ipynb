{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPClassifier():\n",
    "    def __init__(self, hidden_layer_sizes, alpha=0.5, max_iter=100, n_samples=100, min_diff=1e-4):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.n_samples = n_samples\n",
    "        self.min_diff = min_diff\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        K = y.shape[1]\n",
    "        self.topology = [n, self.hidden_layer_sizes, K]\n",
    "        self.L = len(self.topology)\n",
    "        self.Theta = []\n",
    "        self.Delta = []\n",
    "        self.A = []\n",
    "        self.b = []\n",
    "        for s_in, s_out in zip(self.topology, self.topology[1:]):\n",
    "            self.Theta.append(np.random.randn(s_out, s_in))\n",
    "            self.b.append(np.random.randn(s_out))\n",
    "\n",
    "        for s_l in self.topology:\n",
    "            self.A.append(np.empty(s_l))\n",
    "            self.Delta.append(np.empty(s_l)) # we don't use self.Delta[0]\n",
    "        \n",
    "        loss_sum = np.float64(0)\n",
    "        count = 0\n",
    "        for _ in range(0, self.max_iter):\n",
    "            for i in range(0, m):\n",
    "                self.feed_forward(X[i])\n",
    "                self.back_prop(y[i])\n",
    "                loss_sum += self.compute_loss(y[i])\n",
    "                count += 1\n",
    "                if count == self.n_samples:\n",
    "                    if loss_sum / self.n_samples < self.min_diff:\n",
    "                        return self\n",
    "                    count = 0\n",
    "                    loss_sum = np.float64(0)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        s_L = self.topology[self.L-1]\n",
    "        m = X.shape[0]\n",
    "        y = np.empty([m, s_L])\n",
    "        for i in range(0, m):\n",
    "            self.feed_forward(X[i])\n",
    "            y[i][:] = self.A[self.L-1]\n",
    "        return y\n",
    "    \n",
    "    def feed_forward(self, x_i):\n",
    "        self.A[0][:] = x_i\n",
    "        for l in range(0, self.L-1):\n",
    "            z = self.Theta[l] @ self.A[l] + self.b[l]\n",
    "            self.A[l+1][:] = self.g(z)\n",
    "        \n",
    "    # activation function\n",
    "    def g(self, z):\n",
    "        return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "    def back_prop(self, y_i):\n",
    "        self.Delta[self.L-1][:] = self.A[self.L-1] - y_i\n",
    "        for l in range(self.L-2, -1, -1):\n",
    "            gprime = self.A[l] * (1 - self.A[l])\n",
    "            self.Delta[l][:] = gprime * (self.Theta[l].T @ self.Delta[l+1])\n",
    "        # update weights\n",
    "        for l in range(0, self.L-1):\n",
    "            l_in, l_out = self.topology[l:l+2]\n",
    "            self.Theta[l][:] -= self.alpha * self.Delta[l+1].reshape([l_out, 1]) @ self.A[l].reshape([1, l_in])\n",
    "        # update biased terms\n",
    "        for l in range(0, self.L-2):\n",
    "            self.b[l][:] -= self.alpha * self.Delta[l+1]\n",
    "    \n",
    "    def compute_loss(self, y_i):\n",
    "        h = self.A[self.L-1]\n",
    "        return - (y_i @ np.log(h) + (1 - y_i) @ np.log(1 - h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression score: 0.938889\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "n_samples = len(X_digits)\n",
    "n_train = int(.9 * n_samples)\n",
    "\n",
    "X_train = X_digits[:n_train]\n",
    "y_train = y_digits[:n_train]\n",
    "X_test = X_digits[n_train:]\n",
    "y_test = y_digits[n_train:]\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "print('LogisticRegression score: %f' % logistic.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# scale the feature to make the training faster\n",
    "x_max = np.max(X_train)\n",
    "X_train_norm = X_train / x_max\n",
    "X_test_norm = X_test / x_max\n",
    "\n",
    "# transform label to vector\n",
    "y_train_vec = np.array([np.eye(10)[y_i] for y_i in y_train])\n",
    "y_test_vec = np.array([np.eye(10)[y_i] for y_i in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=30, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MLPClassifier at 0x7f0da4906f28>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train_norm, y_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy\n",
    "n_test = len(y_test)\n",
    "h = mlp.predict(X_test_norm)\n",
    "n_count = 0\n",
    "for i in range(0, n_test):\n",
    "    label = np.argmax(h[i])\n",
    "    if label == y_test[i]:\n",
    "        n_count += 1\n",
    "print('Test accuracy: ', n_count / n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accurary:  1.0\n"
     ]
    }
   ],
   "source": [
    "# training accuracy\n",
    "n_train = len(y_train)\n",
    "h = mlp.predict(X_train_norm)\n",
    "n_count = 0\n",
    "for i in range(0, n_train):\n",
    "    label = np.argmax(h[i])\n",
    "    if label == y_train[i]:\n",
    "        n_count += 1\n",
    "print('Training accurary: ', n_count / n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
